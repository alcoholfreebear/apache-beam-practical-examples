{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a6a4adb",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Refs\" data-toc-modified-id=\"Refs-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Refs</a></span></li><li><span><a href=\"#Import-packages\" data-toc-modified-id=\"Import-packages-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Import packages</a></span></li><li><span><a href=\"#Get-data\" data-toc-modified-id=\"Get-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Get data</a></span></li><li><span><a href=\"#Beam-IO\" data-toc-modified-id=\"Beam-IO-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Beam IO</a></span><ul class=\"toc-item\"><li><span><a href=\"#Read-from-bigquery-and-write-to-parquet-in-GCS\" data-toc-modified-id=\"Read-from-bigquery-and-write-to-parquet-in-GCS-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Read from bigquery and write to parquet in GCS</a></span></li></ul></li><li><span><a href=\"#Beam-transformations\" data-toc-modified-id=\"Beam-transformations-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Beam transformations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Aggregations\" data-toc-modified-id=\"Aggregations-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Aggregations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Group-by-and-count\" data-toc-modified-id=\"Group-by-and-count-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Group by and count</a></span></li><li><span><a href=\"#Filter-and-then-count\" data-toc-modified-id=\"Filter-and-then-count-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Filter and then count</a></span></li><li><span><a href=\"#Word-count-then-sort\" data-toc-modified-id=\"Word-count-then-sort-5.1.3\"><span class=\"toc-item-num\">5.1.3&nbsp;&nbsp;</span>Word count then sort</a></span></li></ul></li><li><span><a href=\"#Join:-CoGroupByKey\" data-toc-modified-id=\"Join:-CoGroupByKey-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Join: CoGroupByKey</a></span></li><li><span><a href=\"#Partition\" data-toc-modified-id=\"Partition-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Partition</a></span></li><li><span><a href=\"#Flatten\" data-toc-modified-id=\"Flatten-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Flatten</a></span></li><li><span><a href=\"#Composite-transformation\" data-toc-modified-id=\"Composite-transformation-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Composite transformation</a></span></li></ul></li><li><span><a href=\"#ParDo-transformations\" data-toc-modified-id=\"ParDo-transformations-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>ParDo transformations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pardo-for-Map:-Housing-data\" data-toc-modified-id=\"Pardo-for-Map:-Housing-data-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Pardo for Map: Housing data</a></span></li><li><span><a href=\"#ParDo-for-FlatMap:-Wordcount-revisit\" data-toc-modified-id=\"ParDo-for-FlatMap:-Wordcount-revisit-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>ParDo for FlatMap: Wordcount revisit</a></span></li></ul></li><li><span><a href=\"#Side-inputs-and-side-outputs-in-ParDo\" data-toc-modified-id=\"Side-inputs-and-side-outputs-in-ParDo-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Side inputs and side outputs in ParDo</a></span><ul class=\"toc-item\"><li><span><a href=\"#Side-inputs-in-Pardo\" data-toc-modified-id=\"Side-inputs-in-Pardo-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Side inputs in Pardo</a></span></li><li><span><a href=\"#Side-outputs-in-ParDo\" data-toc-modified-id=\"Side-outputs-in-ParDo-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Side outputs in ParDo</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9cef85",
   "metadata": {},
   "source": [
    "# Refs\n",
    "\n",
    "Beam dataframe colab notebook\n",
    "https://colab.research.google.com/github/apache/beam/blob/master/examples/notebooks/tour-of-beam/dataframes.ipynb#scrollTo=YWYVFkvFuksz\n",
    "\n",
    "Udemy course\n",
    "https://www.udemy.com/course/apache-beam/\n",
    "\n",
    "Good doc\n",
    "https://beam.apache.org/documentation/transforms/python/elementwise/flatmap/\n",
    "\n",
    "\n",
    "\n",
    "# Import packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93959d93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T09:04:16.970575Z",
     "start_time": "2022-09-01T09:04:16.923552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "import pandas as pd\n",
    "import pandas_gbq as gbq\n",
    "import pyarrow.parquet\n",
    "# from pandas_profiling import ProfileReport\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f6eef8",
   "metadata": {},
   "source": [
    "# Get data\n",
    "`../data/housing_short.csv` is created based on \"california housing price\" dataset from Kaggle (https://www.kaggle.com/datasets/camnugent/california-housing-prices.\n",
    "\n",
    "`../data/housing_short.csv` is a shuffled short version of the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "51c55ec8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T11:38:26.054647Z",
     "start_time": "2022-08-08T11:38:25.381715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longitude,latitude,housing_median_age,total_rooms,total_bedrooms,population,households,median_income,median_house_value,ocean_proximity\r\n",
      "-121.7,36.6,19.0,3562.0,530.0,1607.0,505.0,5.0162,283100.0,<1H OCEAN\r\n",
      "-118.3,34.01,48.0,4217.0,1095.0,3298.0,949.0,1.9152,122300.0,<1H OCEAN\r\n",
      "-121.75,36.92,46.0,1362.0,321.0,1068.0,305.0,2.4615,177800.0,<1H OCEAN\r\n",
      "-117.92,33.92,19.0,2181.0,400.0,1272.0,337.0,5.1952,302100.0,<1H OCEAN\r\n",
      "-122.05,37.36,34.0,2400.0,419.0,1017.0,384.0,4.1369,316900.0,<1H OCEAN\r\n",
      "-118.86,34.16,16.0,1509.0,216.0,578.0,235.0,10.2614,410800.0,<1H OCEAN\r\n",
      "-117.96,33.99,25.0,1348.0,210.0,660.0,200.0,5.2852,297600.0,<1H OCEAN\r\n",
      "-118.16,34.12,38.0,2231.0,489.0,940.0,484.0,5.4165,435100.0,<1H OCEAN\r\n",
      "-117.02,32.71,30.0,3187.0,592.0,2082.0,631.0,3.5388,118500.0,NEAR OCEAN\r\n"
     ]
    }
   ],
   "source": [
    "!head ../data/housing_short.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6b9dc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-03T12:54:49.697878Z",
     "start_time": "2022-08-03T12:54:49.590750Z"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "946185ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T11:38:40.537027Z",
     "start_time": "2022-08-08T11:38:40.353457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 120 entries, 0 to 119\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   longitude           120 non-null    float64\n",
      " 1   latitude            120 non-null    float64\n",
      " 2   housing_median_age  120 non-null    float64\n",
      " 3   total_rooms         120 non-null    float64\n",
      " 4   total_bedrooms      118 non-null    float64\n",
      " 5   population          120 non-null    float64\n",
      " 6   households          120 non-null    float64\n",
      " 7   median_income       120 non-null    float64\n",
      " 8   median_house_value  120 non-null    float64\n",
      " 9   ocean_proximity     120 non-null    object \n",
      "dtypes: float64(9), object(1)\n",
      "memory usage: 9.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/housing_short.csv')\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76dab6ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-03T12:56:45.488324Z",
     "start_time": "2022-08-03T12:56:45.400259Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ocean_proximity\n",
       "<1H OCEAN     9136\n",
       "INLAND        6551\n",
       "ISLAND           5\n",
       "NEAR BAY      2290\n",
       "NEAR OCEAN    2658\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('ocean_proximity').size()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93a9549",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-03T07:02:45.355723Z",
     "start_time": "2022-08-03T07:02:44.871989Z"
    }
   },
   "source": [
    "# Beam IO\n",
    "## Read from bigquery and write to parquet in GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527db7a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T09:28:24.800560Z",
     "start_time": "2022-09-01T09:28:24.132270Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "191c08c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T09:10:01.642875Z",
     "start_time": "2022-09-01T09:10:01.572590Z"
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT county_name, county_fips_code, state_name \n",
    "FROM `bigquery-public-data.covid19_public_forecasts.county_14d` \n",
    "WHERE prediction_date >= \"2021-09-01\" \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb735b7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T09:10:03.781366Z",
     "start_time": "2022-09-01T09:10:01.939193Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|███████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 32.94rows/s]\n"
     ]
    }
   ],
   "source": [
    "df = gbq.read_gbq(query+\" limit 10\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26d078a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T09:08:11.830298Z",
     "start_time": "2022-09-01T09:08:11.726854Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path='../data/df_s.parquet'\n",
    "df.to_parquet(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eba24833",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T09:53:55.684222Z",
     "start_time": "2022-09-01T09:53:55.364629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "county_name: string\n",
      "county_fips_code: string\n",
      "state_name: string\n",
      "-- schema metadata --\n",
      "pandas: '{\"index_columns\": [{\"kind\": \"range\", \"name\": null, \"start\": 0, \"' + 633\n"
     ]
    }
   ],
   "source": [
    "schema=pyarrow.parquet.read_schema(data_path)\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "547e0af7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T09:13:46.945626Z",
     "start_time": "2022-09-01T09:13:46.896563Z"
    }
   },
   "outputs": [],
   "source": [
    "bucket_name='beam-learn'\n",
    "project_id='XXX'\n",
    "region='us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4e63ca3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T09:24:43.166505Z",
     "start_time": "2022-09-01T09:19:02.185722Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/Users/shuyili/Library/Jupyter/runtime/kernel-8d0875fd-950f-4a40-b95b-170cc8ef9968.json']\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/Users/shuyili/Library/Jupyter/runtime/kernel-8d0875fd-950f-4a40-b95b-170cc8ef9968.json']\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/Users/shuyili/Library/Jupyter/runtime/kernel-8d0875fd-950f-4a40-b95b-170cc8ef9968.json']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "beam_options=PipelineOptions(project=project_id, \n",
    "                             runner='DataflowRunner',\n",
    "                             # runner='DirectRunner',\n",
    "                             temp_location=f'gs://{bucket_name}/temp',\n",
    "                             job_name = 'save-data-to-parquet',\n",
    "                             region=region,\n",
    "                             \n",
    "                            )\n",
    "\n",
    "file_path_prefix=f'gs://{bucket_name}/data/beam-output/small-parquet-pc/bq'\n",
    "# file_path_prefix='../outputs/bqdata'\n",
    "\n",
    "with beam.Pipeline(options=beam_options) as p:\n",
    "    test_write = (\n",
    "        p\n",
    "        | 'Read BigQuery' >>  beam.io.ReadFromBigQuery( #beam.io.Read(beam.io.BigQuerySource(\n",
    "            query=query+' limit 1000',\n",
    "            use_standard_sql=True)\n",
    "        #| beam.combiners.ToList()\n",
    "        # Each row is a dictionary where the keys are the BigQuery columns\n",
    "        | 'Save as parquet to GCS' >> beam.io.parquetio.WriteToParquet(file_path_prefix=file_path_prefix, \n",
    "                                           schema=pyarrow.schema(schema), \n",
    "                                           record_batch_size=1000,\n",
    "                                           file_name_suffix='.parquet', num_shards=10))\n",
    "        #| beam.combiners.ToList()\n",
    "        #| beam.Map(print))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f38fdf",
   "metadata": {},
   "source": [
    "# Beam transformations\n",
    "\n",
    "## Aggregations\n",
    "### Group by and count\n",
    "Doing the same group by 'ocean_proximity' and count operation using beam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "83750f93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T11:39:21.691488Z",
     "start_time": "2022-08-08T11:39:21.199824Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline() as p:\n",
    "    near_ocean_count=(\n",
    "        p \n",
    "        | beam.io.ReadFromText('../data/housing_short.csv', skip_header_lines=1)\n",
    "        | beam.Map(lambda x: x.split(',')) # every step is a pCollection\n",
    "        | beam.Map(lambda x: (x[9], 1))\n",
    "        | beam.CombinePerKey(sum)\n",
    "        | beam.io.WriteToText('../outputs/ocean_proximity_agg_count.txt')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "352325d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T11:39:23.626805Z",
     "start_time": "2022-08-08T11:39:22.951328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<1H OCEAN', 51)\r\n",
      "('NEAR OCEAN', 14)\r\n",
      "('INLAND', 45)\r\n",
      "('NEAR BAY', 9)\r\n",
      "('ISLAND', 1)\r\n"
     ]
    }
   ],
   "source": [
    "!cat '../outputs/ocean_proximity_agg_count.txt-00000-of-00001'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fdf06e",
   "metadata": {},
   "source": [
    "### Filter and then count\n",
    "Count when 'ocean_proximity' value is 'NEAR OCEAN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "769a55a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T11:39:46.718001Z",
     "start_time": "2022-08-08T11:39:46.191079Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
     ]
    }
   ],
   "source": [
    "# One can set runner='DirectRunner' or 'DataflowRunner'\n",
    "with beam.Pipeline() as p:\n",
    "    near_ocean_count=(\n",
    "        p \n",
    "        | beam.io.ReadFromText('../data/housing_short.csv', skip_header_lines=1, min_bundle_size=1024)\n",
    "        | beam.Map(lambda x: x.split(','))\n",
    "        | beam.Filter(lambda x: x[9]=='NEAR OCEAN') # add filter here\n",
    "        | beam.Map(lambda x: (x[9], 1))\n",
    "        | beam.CombinePerKey(sum)\n",
    "        | beam.io.WriteToText('../outputs/near_ocean_count.txt')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "98058e24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T11:39:47.387659Z",
     "start_time": "2022-08-08T11:39:46.723538Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('NEAR OCEAN', 14)\r\n"
     ]
    }
   ],
   "source": [
    "# check newly generated output file\n",
    "!cat '../outputs/near_ocean_count.txt-00000-of-00001'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff469661",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-03T07:14:08.432194Z",
     "start_time": "2022-08-03T07:14:07.769709Z"
    }
   },
   "source": [
    "### Word count then sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b90e265a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-03T14:43:16.347815Z",
     "start_time": "2022-08-03T14:43:16.249472Z"
    }
   },
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "Peter Piper picked a peck of pickled peppers,\n",
    "A peck of pickled peppers Peter Piper picked;\n",
    "If Peter Piper picked a peck of pickled peppers,\n",
    "Where’s the peck of pickled peppers Peter Piper picked?\n",
    "\"\"\"\n",
    "with open('../data/peter_piper.txt', 'w') as f:\n",
    "    f.write(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "539be958",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-03T14:43:24.888470Z",
     "start_time": "2022-08-03T14:43:24.189733Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Peter Piper picked a peck of pickled peppers,\r\n",
      "A peck of pickled peppers Peter Piper picked;\r\n",
      "If Peter Piper picked a peck of pickled peppers,\r\n",
      "Where’s the peck of pickled peppers Peter Piper picked?\r\n"
     ]
    }
   ],
   "source": [
    "cat ../data/peter_piper.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b69f1df0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-05T07:13:35.503127Z",
     "start_time": "2022-08-05T07:13:34.762454Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
     ]
    }
   ],
   "source": [
    "def sort_by_key(record):\n",
    "    record.sort(key=lambda x: x[0], reverse=False)\n",
    "    return record\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    word_count=(\n",
    "        p \n",
    "        | 'Read input' >> beam.io.ReadFromText('../data/peter_piper.txt')\n",
    "        | 'Split by space' >> beam.FlatMap(lambda x: x.split())\n",
    "        | 'Strip symbols' >> beam.Map(lambda x: x.strip(',;?').lower())\n",
    "        # | beam.Filter(lambda x: x=='Peter') # add filter here\n",
    "        | beam.Map(lambda x: (x, 1))\n",
    "        | beam.CombinePerKey(sum)\n",
    "        | beam.combiners.ToList()\n",
    "        | beam.Map(sort_by_key) # This works after flattening \n",
    "                                # by changing to list, beam.combiners.ToList():\n",
    "                                # [[ele],[ele],[ele]] => [ele, ele, ele] \n",
    "                                # beam.Flatten() didn't work  \n",
    "        | beam.io.WriteToText('../outputs/peter_count.txt')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8cd863b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-05T14:49:21.124701Z",
     "start_time": "2022-08-05T14:49:19.099726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 3), ('if', 1), ('of', 4), ('peck', 4), ('peppers', 4), ('peter', 4), ('picked', 4), ('pickled', 4), ('piper', 4), ('the', 1), ('where’s', 1)]\r\n"
     ]
    }
   ],
   "source": [
    "cat '../outputs/peter_count.txt-00000-of-00001'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a45c1a",
   "metadata": {},
   "source": [
    "## Join: CoGroupByKey\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c5b4d20b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-05T06:16:45.922450Z",
     "start_time": "2022-08-05T06:16:45.399588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, (['SpiderMan'], [3.5]))\n",
      "(2, (['Avenger'], [4.5]))\n",
      "(3, (['Titanic'], [3.5]))\n",
      "(4, (['Green Miles'], [4.5]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<apache_beam.runners.portability.fn_api_runner.fn_runner.RunnerResult at 0x7fc3a3ba1880>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = beam.Pipeline()\n",
    "movie_name = [\n",
    "    (1, 'SpiderMan'),\n",
    "    (2, 'Avenger'),\n",
    "    (3, 'Titanic'),\n",
    "    (4, 'Green Miles'),\n",
    "]\n",
    "\n",
    "movie_rating = [\n",
    "    (1, 3.5),\n",
    "    (2, 4.5),\n",
    "    (3, 3.5),\n",
    "    (4, 4.5),\n",
    "]\n",
    "\n",
    "name = p | 'Create Name Pcollection' >> beam.Create(movie_name)\n",
    "rating = p | 'Create Rating Pcollection' >> beam.Create(movie_rating)\n",
    "\n",
    "join_result = ({name, rating} | beam.CoGroupByKey()) | beam.Map(print)\n",
    "p.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bb06b8",
   "metadata": {},
   "source": [
    "## Partition\n",
    "Split one Pcollection into many Pcollections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0977567a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-05T06:30:28.860914Z",
     "start_time": "2022-08-05T06:30:28.615032Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "6\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<apache_beam.runners.portability.fn_api_runner.fn_runner.RunnerResult at 0x7fc3a3bac6d0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = beam.Pipeline()\n",
    "number = {1,2,3,4,5,6,7,8,9,10}\n",
    "\n",
    "def partition_fn(element, num_partition):\n",
    "    return element%num_partition\n",
    "\n",
    "number_pc = p | beam.Create(number) | beam.Partition(partition_fn, 4)\n",
    "\n",
    "number_pc[2] | 'Print the first Pcollection' >> beam.Map(print)\n",
    "\n",
    "p.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d61c73",
   "metadata": {},
   "source": [
    "## Flatten\n",
    "\n",
    "Take many Pcollections and make them one Pcollection.\n",
    "\n",
    "Note: The output has randomized orders of the input Pcollections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d0f93026",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-05T06:45:04.352875Z",
     "start_time": "2022-08-05T06:45:03.905575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "john\n",
      "jim\n",
      "mary\n",
      "8\n",
      "2\n",
      "4\n",
      "6\n",
      "1\n",
      "3\n",
      "5\n",
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<apache_beam.runners.portability.fn_api_runner.fn_runner.RunnerResult at 0x7fc3a5380fa0>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = beam.Pipeline()\n",
    "\n",
    "even={2,4,6,8}\n",
    "odd = {1,3,5,7}\n",
    "name = ('john', 'jim', 'mary')\n",
    "\n",
    "even_pc = p | 'Create Pcollection from even numbers' >> beam.Create(even)\n",
    "odd_pc = p | 'Create Pcollection from odd numbers' >> beam.Create(odd)\n",
    "name_pc = p | 'Create Pcollection from names' >> beam.Create(name)\n",
    "\n",
    "result = (even_pc, odd_pc, name_pc) | 'Flatten' >> beam.Flatten() | beam.Map(print)\n",
    "\n",
    "p.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf12bca",
   "metadata": {},
   "source": [
    "## Composite transformation\n",
    "\n",
    "Combine multiple beam transformations in a single custom PTransform class. \n",
    "This is especially useful to avoid repeating codes.\n",
    "\n",
    "For the previous example in 3.1.1:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "791a1db4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T11:40:09.561077Z",
     "start_time": "2022-08-08T11:40:05.057356Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<1H OCEAN', 51)\n",
      "('NEAR OCEAN', 14)\n",
      "('INLAND', 45)\n",
      "('NEAR BAY', 9)\n",
      "('ISLAND', 1)\n"
     ]
    }
   ],
   "source": [
    "# Old code \n",
    "with beam.Pipeline() as p:\n",
    "    near_ocean_count=(\n",
    "        p \n",
    "        | beam.io.ReadFromText('../data/housing_short.csv', skip_header_lines=1)\n",
    "        | beam.Map(lambda x: x.split(',')) # output of every step is a pCollection\n",
    "        | beam.Map(lambda x: (x[9], 1))\n",
    "        | beam.CombinePerKey(sum)\n",
    "        | beam.Map(print)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "5ebb769d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T11:40:49.247485Z",
     "start_time": "2022-08-08T11:40:48.967117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<1H OCEAN', 51)\n",
      "('NEAR OCEAN', 14)\n",
      "('INLAND', 45)\n",
      "('NEAR BAY', 9)\n",
      "('ISLAND', 1)\n"
     ]
    }
   ],
   "source": [
    "# New code packing several PTransforms inside of a class\n",
    "\n",
    "class CombinedTransformation(beam.PTransform):\n",
    "    # overwrite the expand() method\n",
    "    def expand(self, input_pcollection):\n",
    "        a = (\n",
    "            input_pcollection\n",
    "                                | beam.Map(lambda x: x.split(',')) # every step is a pCollection\n",
    "                                | beam.Map(lambda x: (x[9], 1))\n",
    "                                | beam.CombinePerKey(sum) \n",
    "        )\n",
    "        return a\n",
    "    \n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    near_ocean_count=(\n",
    "        p \n",
    "        | beam.io.ReadFromText('../data/housing_short.csv', skip_header_lines=1)\n",
    "        | \"Combined PTransform\" >> CombinedTransformation()\n",
    "        | beam.Map(print)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9f672f",
   "metadata": {},
   "source": [
    "# ParDo transformations\n",
    "Transformations such as Map and FlatMap are \"ParDo\"s. \n",
    "\n",
    "Write custom ParDo by `beam.ParDo( custome_dofn_class, arg=arg)`, obviously you need to write your own `DoFn` class `custome_dofn_class`.\n",
    "\n",
    "To write custome `DoFn`, overwrite the `process` function.\n",
    "\n",
    "## Pardo for Map: Housing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "d86b3f23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T11:41:39.849681Z",
     "start_time": "2022-08-08T11:41:39.604725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-124.06', '40.86', '34.0', '4183.0', '', '1891.0', '669.0', '3.2216', '98100.0', 'NEAR OCEAN']\n"
     ]
    }
   ],
   "source": [
    "# Pardo for Map\n",
    "# FlatMap: DoFn returns list\n",
    "\n",
    "class SplitRow(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        return [element.split(',')]\n",
    "\n",
    "class FilterItems(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        if element[9]=='NEAR OCEAN' and float(element[8])<100000:\n",
    "            return [element]\n",
    "\n",
    "        \n",
    "with beam.Pipeline() as p:\n",
    "    near_ocean_count=(\n",
    "        p \n",
    "        | \"Read File\" >> beam.io.ReadFromText('../data/housing_short.csv', skip_header_lines=1)\n",
    "        | \"Split\" >> beam.ParDo(SplitRow())\n",
    "        | \"Filter Customer\" >> beam.ParDo(FilterItems())\n",
    "        | \"Print\" >> beam.Map(print)\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08d2885",
   "metadata": {},
   "source": [
    "## ParDo for FlatMap: Wordcount revisit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a302da1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T08:51:17.418707Z",
     "start_time": "2022-08-08T08:51:16.803704Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('peter', 4)\n",
      "('picked', 4)\n",
      "('piper', 4)\n"
     ]
    }
   ],
   "source": [
    "# Pardo for FlatMap\n",
    "# FlatMap: DoFn returns element\n",
    "# Use generator to unflatten\n",
    "\n",
    "class SplitRow(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        return element.split()\n",
    "    \n",
    "class StripLower(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        return [element.strip(',;?').lower()]\n",
    "    \n",
    "def find_word(element):\n",
    "    words = ['peter', 'piper', 'picked']\n",
    "    if element in words:\n",
    "        return True\n",
    "\n",
    "def sort_by_key(record):\n",
    "    \"\"\"make into generator\n",
    "    \"\"\"\n",
    "    record.sort(key=lambda x: x[0], reverse=False)\n",
    "    for x in record:\n",
    "        yield x\n",
    "        \n",
    "class SumValuesByKey(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        (key, value) = element\n",
    "        return [(key, sum(value))]\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    word_count=(\n",
    "        p \n",
    "        | \"Read file\" >> beam.io.ReadFromText('../data/peter_piper.txt')\n",
    "        | \"Split row - flatmap\" >> beam.ParDo(SplitRow()) # FlatMap\n",
    "        | \"Strip and lower words - map\" >> beam.ParDo(StripLower()) # Map\n",
    "        | \"Find word\" >> beam.Filter(find_word) # Map        \n",
    "        | \"Attach 1 count\" >> beam.Map(lambda x: (x, 1))\n",
    "        | \"Make list of ones per word\" >> beam.GroupByKey() \n",
    "                            # step 1 of old beam.CombinePerKey(sum)\n",
    "                            # outputs:\n",
    "                                    # ('peter', [1,1,1,1])\n",
    "                                    # ('picked', [1,1,1,1])\n",
    "                                    # ('piper', [1,1,1,1])\n",
    "        | \"Sum the ones per key\" >> beam.ParDo(SumValuesByKey()) \n",
    "                            # step 2 of old beam.CombinePerKey(sum)\n",
    "                            # outputs:\n",
    "                                    # ('peter', 4)\n",
    "                                    # ('picked', 4)\n",
    "                                    # ('piper', 4)        \n",
    "        | \"Turn into list\" >> beam.combiners.ToList()\n",
    "        | \"Sort by key\" >> beam.FlatMap(sort_by_key) # Use Generator to \"unflatten\" the list. \n",
    "                                    # This solves the problem in previous section 4.1.3\n",
    "        | \"Print\" >> beam.Map(print)\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b539116",
   "metadata": {},
   "source": [
    "# Side inputs and side outputs in ParDo\n",
    "## Side inputs in Pardo\n",
    "To use side inputs in ParDo, modify the corresponding DoFn to read side inputs. \n",
    "\n",
    "Example using housing data to attach a labe by `ocean_proximity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "c912a23a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T11:42:24.240548Z",
     "start_time": "2022-08-08T11:42:23.987814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-124.06', '40.86', '34.0', '4183.0', '', '1891.0', '669.0', '3.2216', '98100.0', 'NEAR OCEAN', 'premium']\n"
     ]
    }
   ],
   "source": [
    "# side_input is labels\n",
    "# alternative side_input can be a list of items ids that should be excluded etc.\n",
    "# Or overwrite part of the items etc.\n",
    "\n",
    "labels=[\n",
    "('NEAR BAY', 'premium'),\n",
    "('<1H OCEAN', 'good'),\n",
    "('INLAND', 'common'),\n",
    "('NEAR OCEAN', 'premium'),\n",
    "('ISLAND', 'super premium')]\n",
    "\n",
    "\n",
    "class SplitRow(beam.DoFn):\n",
    "    def process(self, element, side_input):\n",
    "        rows = element.split(',') \n",
    "        return [rows + [dict( tuple(labels))[rows[9]] ]] \n",
    "\n",
    "\n",
    "class FilterItems(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        if element[9]=='NEAR OCEAN' and float(element[8])<100000:\n",
    "            return [element]\n",
    "\n",
    "\n",
    "        \n",
    "with beam.Pipeline() as p:\n",
    "    near_ocean_count=(\n",
    "        p \n",
    "        | \"Read File\" >> beam.io.ReadFromText('../data/housing_short.csv', skip_header_lines=1)\n",
    "        | \"Split\" >> beam.ParDo(SplitRow(), side_input=labels)\n",
    "        | \"Filter Customer\" >> beam.ParDo(FilterItems())\n",
    "        | \"Print\" >> beam.Map(print)\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0177743f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T09:12:21.852401Z",
     "start_time": "2022-08-08T09:12:21.785492Z"
    }
   },
   "source": [
    "## Side outputs in ParDo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "bbd3f27d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T10:08:08.745140Z",
     "start_time": "2022-08-08T10:08:07.936148Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n",
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
     ]
    }
   ],
   "source": [
    "\n",
    "funny_words = ['peter', 'piper', 'picked']\n",
    "\n",
    "\n",
    "class ProcessWords(beam.DoFn):\n",
    "    def process(self, element, side_input):\n",
    "        if element[0] in side_input:\n",
    "            yield element\n",
    "        else:\n",
    "            yield beam.pvalue.TaggedOutput('boring', element)\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    word_count=(\n",
    "        p \n",
    "        | \"Read file\" >> beam.io.ReadFromText('../data/peter_piper.txt')\n",
    "        | \"Split row - flatmap\" >> beam.FlatMap(lambda x: x.split()) # FlatMap\n",
    "        | \"Strip and lower words - map\" >> beam.Map(lambda x: x.strip(',;?').lower()) # Map\n",
    "        | \"Attach 1 count\" >> beam.Map(lambda x: (x, 1))\n",
    "        | \"Count frequency per word\" >> beam.CombinePerKey(sum) \n",
    "        | \"Section word count\" >> beam.ParDo(ProcessWords(), side_input=funny_words).with_outputs('boring', main='funny')\n",
    "    ) \n",
    "\n",
    "    # funny tagged\n",
    "    word_count.funny | 'Write funny wordcounts' >> beam.io.WriteToText('../outputs/funny_wordcount.txt')\n",
    "    # boring tagged\n",
    "    word_count.boring | 'Write boring wordcounts' >> beam.io.WriteToText('../outputs/boring_wordcount.txt')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "b311a7a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T10:08:09.446114Z",
     "start_time": "2022-08-08T10:08:08.748910Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('peter', 4)\r\n",
      "('piper', 4)\r\n",
      "('picked', 4)\r\n"
     ]
    }
   ],
   "source": [
    "cat '../outputs/funny_wordcount.txt-00000-of-00001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "8c911998",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T10:09:22.691655Z",
     "start_time": "2022-08-08T10:09:21.834059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 3)\r\n",
      "('peck', 4)\r\n",
      "('of', 4)\r\n",
      "('pickled', 4)\r\n",
      "('peppers', 4)\r\n",
      "('if', 1)\r\n",
      "('where’s', 1)\r\n",
      "('the', 1)\r\n"
     ]
    }
   ],
   "source": [
    "cat '../outputs/boring_wordcount.txt-00000-of-00001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cc0125",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beam-learn",
   "language": "python",
   "name": "beam-learn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
